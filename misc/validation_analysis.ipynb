{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Validation Analysis\n",
    "#### David da Costa Correia @ FCUL & INSA\n",
    "\n",
    "1. What this notebook does:\n",
    "\t1. Create a validated relation dataset\n",
    "\t\t1. Read forms JSON file (see validation/create_validation_forms.py)\n",
    "\t\t2. Merge relations and human curated validation\n",
    "\t\t3. Create DataFrame and output it to CSV file\n",
    "\t2. Validated corpus analysis\n",
    "\t\t1. Curator Evaluation\n",
    "\t\t2. Corpus Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMS_FILE = '../outputs/validation/form_responses.json'\n",
    "OUTPUT_FILE = '../outputs/dataset/val_rel_dataset.csv'\n",
    "OVERLAP_SAMPLE_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FORMS_FILE, 'r') as f:\n",
    "\tforms = json.load(f)\n",
    "\n",
    "overlap_rels = [{} for _ in range(OVERLAP_SAMPLE_SIZE)] # [{\"ID\":str, \"sentence\":str, \"e1\":str, \"e1_id\":str, \"e2\":str, \"e2_id\":str, \"pred\":int, \"eval\":list}]\n",
    "rels = [] # [{\"ID\":str, \"sentence\":str, \"e1\":str, \"e1_id\":str, \"e2\":str, \"e2_id\":str, \"pred\":int, \"eval\":str}]\n",
    "\n",
    "overlap_done = False # checks if overlap_rels was initialized\n",
    "for form in forms.values():\n",
    "\tif form[\"responses\"] is None:\n",
    "\t\tcontinue\n",
    "\tfor i,relation in enumerate(form[\"relations\"]):\n",
    "\t\tsentence, meta = relation\n",
    "\t\trel = {\n",
    "\t\t\t\"ID\":'',\n",
    "\t\t\t\"sentence\":sentence,\n",
    "\t\t\t\"e1\":meta[\"e1\"][\"text\"],\n",
    "\t\t\t\"e1_ID\":meta[\"e1\"][\"ID\"],\n",
    "\t\t\t\"e2\":meta[\"e2\"][\"text\"],\n",
    "\t\t\t\"e2_ID\":meta[\"e2\"][\"ID\"],\n",
    "\t\t\t\"pred\":meta[\"relation\"],\n",
    "\t\t\t\"eval\":None\n",
    "\t\t}\n",
    "\t\t# if it is an overlap relation\n",
    "\t\tif i < OVERLAP_SAMPLE_SIZE:\n",
    "\t\t\tif not overlap_done:\n",
    "\t\t\t\t# Setup overlap_rels\n",
    "\t\t\t\trel[\"ID\"] = f\"Ov{i+1}\"\n",
    "\t\t\t\trel[\"eval\"] = [form[\"responses\"][i][0]]\n",
    "\t\t\t\toverlap_rels[i] = rel\n",
    "\t\t\telse:\n",
    "\t\t\t\toverlap_rels[i][\"eval\"].append(form[\"responses\"][i][0])\n",
    "\t\t# if it is an unique relation\n",
    "\t\telse:\n",
    "\t\t\trel[\"ID\"] = len(rels)+1\n",
    "\t\t\trel[\"eval\"] = form[\"responses\"][i][0]\n",
    "\t\t\trels.append(rel)\n",
    "\n",
    "\toverlap_done = True\n",
    "\n",
    "val_dataset = pd.DataFrame(rels)\n",
    "val_dataset.to_csv(OUTPUT_FILE, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validated Corpus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATED_DATASET_FILE = OUTPUT_FILE\n",
    "# VALIDATED_DATASET_FILE = './outputs/dataset/val_rel_dataset1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluator metrics\n",
    "def fleiss_kappa(evals:dict, categories):\n",
    "    # Following https://en.wikipedia.org/wiki/Fleiss%27_kappa\n",
    "    N = len(evals) # number of subjects\n",
    "    n = len(list(evals.values())[0]) # number of raters\n",
    "    \n",
    "    ps = []\n",
    "    for j in categories:\n",
    "        S = sum([evals[i].count(j) for i in evals])\n",
    "        p_j = (1/(N*n))*S\n",
    "        ps.append(p_j)\n",
    "    # print(ps)\n",
    "\n",
    "    Ps = []\n",
    "    for i in evals:\n",
    "        S = sum([(evals[i].count(j))**2 for j in categories]) - n\n",
    "        P_i = (1/(n*(n-1)))*S\n",
    "        Ps.append(P_i)\n",
    "    # print(Ps)\n",
    "\n",
    "    Pe_bar = sum([p**2 for p in ps])\n",
    "    P_bar = np.mean(Ps)\n",
    "    # print(P_bar)\n",
    "\n",
    "    return (P_bar-Pe_bar)/(1-Pe_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "overlap_evals = {rel['ID']:rel['eval'] for rel in overlap_rels}\n",
    "original_kappa = fleiss_kappa(overlap_evals,['C','I','U'])\n",
    "n_raters = len(list(overlap_evals.values())[0])\n",
    "print(f\"Baseline Fleiss' Kappa: {original_kappa:.3f}\")\n",
    "\n",
    "# 1. Pairwise Agreement (Cohen's Kappa)\n",
    "# df = pd.DataFrame(overlap_evals, columns=None, index=None).T\n",
    "# pairwise_kappas = np.zeros((n_raters, n_raters))\n",
    "# for i in range(n_raters):\n",
    "#     for j in range(n_raters):\n",
    "#         if i == j:\n",
    "#             pairwise_kappas[i, j] = 1  # Self-agreement is always 1\n",
    "#         else:\n",
    "#             pairwise_kappas[i, j] = cohen_kappa_score(df.iloc[:, i], df.iloc[:, j])\n",
    "# pairwise_kappas = pd.DataFrame(pairwise_kappas)\n",
    "\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(pairwise_kappas, annot=True, cmap='coolwarm', center=0)\n",
    "# plt.gca().xaxis.tick_top()\n",
    "# plt.title('Pairwise Cohen\\'s Kappa')\n",
    "# plt.show()\n",
    "\n",
    "# 2. Leave-One-Out Analysis\n",
    "loo_kappas = []\n",
    "for i in range(n_raters):\n",
    "    evals = {rel:[rel_evals[j] for j in range(n_raters) if i != j] for rel,rel_evals in overlap_evals.items()}\n",
    "    loo_kappas.append(fleiss_kappa(evals, ['C','I','U']))\n",
    "\n",
    "loo_deltas = [original_kappa-k for k in loo_kappas]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(range(1,len(loo_deltas)+1), loo_deltas, marker='o', color='black')\n",
    "# plt.title('Leave-One-Out Analysis of Fleiss\\' Kappa')\n",
    "plt.axhline(y=0, color='r', linestyle='-', label=f'$k_{0}$ = {original_kappa:.3f}')\n",
    "plt.xticks([i for i in range(1,11)])\n",
    "plt.yticks([i*0.01 for i in range(-10,4,2)])\n",
    "plt.xlabel('Removed curator')\n",
    "plt.ylabel('$k_{0}$ - $k_{LOO}$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, f1\n",
    "def confusion_matrix(preds, evals):\n",
    "    tp, tn, fp, fn, unc = 0, 0, 0, 0, 0\n",
    "    for i,eva in enumerate(evals):\n",
    "        if eva == 'U': unc += 1\n",
    "        elif eva == 'C' and preds[i] == 1: tp += 1\n",
    "        elif eva == 'I' and preds[i] == 1: fp += 1\n",
    "        elif eva == 'C' and preds[i] == 0: tn += 1\n",
    "        elif eva == 'I' and preds[i] == 0: fn += 1\n",
    "    return (tp, fp, tn, fn, unc)\n",
    "\n",
    "def score(TP, FP, FN):\n",
    "    prc = TP/(TP+FP)\n",
    "    rcl = TP/(TP+FN)\n",
    "    return prc, rcl, 2*(prc*rcl)/(prc+rcl) if prc+rcl != 0 else 0\n",
    "\n",
    "def drop_raters(df:pd.DataFrame, raters:list):\n",
    "\tout_df = df.copy(deep=True)\n",
    "\tfor rater in raters:\n",
    "\t\tout_df = out_df.drop(range(rater*20,rater*20+20))\n",
    "\treturn out_df\n",
    "\n",
    "RATER_DROP_THRESHOLD = 0.05\n",
    "droped_raters = [loo_deltas.index(delta) for delta in loo_deltas if abs(delta) >= RATER_DROP_THRESHOLD]\n",
    "# droped_raters = []\n",
    "clean_val_dataset = drop_raters(val_dataset, droped_raters)\n",
    "preds = clean_val_dataset['pred'].tolist()\n",
    "evals = clean_val_dataset['eval'].tolist()\n",
    "\n",
    "cm = confusion_matrix(preds,evals)\n",
    "tp, fp, tn, fn, unc = cm\n",
    "prc, rcl, f1 = score(tp,fp,fn)\n",
    "filtered_evals = {rel:[rel_evals[i] for i in range(n_raters) if i not in droped_raters] for rel,rel_evals in overlap_evals.items()}\n",
    "k = fleiss_kappa(filtered_evals, ['C','I','U'])\n",
    "print(f\"Number of validated relations: {sum(cm)}\")\n",
    "print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "print(f\"Precision: {prc:.3f}\")\n",
    "print(f\"Recall:    {rcl:.3f}\")\n",
    "print(f\"F1:        {f1:.3f}\")\n",
    "print(f\"Fleiss' K: {k:.3f}\")\n",
    "print(f\"Number of Uncertains: {unc} ({unc/sum(cm)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATER_DROP_THRESHOLD = 0.1\n",
    "PATH = '../llms/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Test Set\n",
    "import os\n",
    "\n",
    "def get_true_label(row):\n",
    "\tif row['eval'] == 'U': return 'U'\n",
    "\tif row['eval'] == 'C': return row['pred']\n",
    "\t# if pred == 1 and is Incorrect: return 0 and vice-versa\n",
    "\tif row['eval'] == 'I': return int(not bool(row['pred'])) \n",
    "\n",
    "droped_raters = [loo_deltas.index(delta) for delta in loo_deltas if delta <= RATER_DROP_THRESHOLD]\n",
    "clean_val_dataset = drop_raters(val_dataset, droped_raters)\n",
    "\n",
    "clean_val_dataset['true_label'] = clean_val_dataset.apply(get_true_label, axis=1)\n",
    "clean_val_dataset.drop([\"pred\",\"eval\"], axis=1, index=None, inplace=True)\n",
    "pos = clean_val_dataset[clean_val_dataset['true_label'] == 1]\n",
    "neg = clean_val_dataset[clean_val_dataset['true_label'] == 0]\n",
    "unc = clean_val_dataset[clean_val_dataset['true_label'] == 'U']\n",
    "\n",
    "train_p = 0.7\n",
    "pos_size = round(len(pos)*train_p)\n",
    "neg_size = round(len(neg)*train_p)\n",
    "unc_size = round(len(unc)*train_p)\n",
    "\n",
    "if not os.path.exists(PATH+'train.csv'):\n",
    "\ttrain = pd.concat([pos[:pos_size],neg[:pos_size]]).sort_values(by='ID')\n",
    "\ttrain.to_csv(PATH+'train.csv', sep='\\t', index=None)\n",
    "\t\n",
    "\ttest = pd.concat([pos[pos_size:],neg[pos_size:]]).sort_values(by='ID')\n",
    "\ttest.to_csv(PATH+'test.csv', sep='\\t', index=None)\n",
    "\t\n",
    "\ttrain_u = pd.concat([train,unc[:unc_size]]).sort_values(by='ID')\n",
    "\ttrain_u.to_csv(PATH+'train_u.csv', sep='\\t', index=None)\n",
    "\n",
    "\ttest_u = pd.concat([test,unc[unc_size:]]).sort_values(by='ID')\n",
    "\ttest_u.to_csv(PATH+'test_u.csv', sep='\\t', index=None)\n",
    "\n",
    "else: # Add new rows to the test\n",
    "\t# get new rows that are not in the train set\n",
    "\ttrain_ids = list(pd.read_csv(PATH+'train_u.csv', sep='\\t')['ID'])\n",
    "\ttest_ids = list(pd.read_csv(PATH+'test_u.csv', sep='\\t')['ID'])\n",
    "\told_ids = train_ids+test_ids\n",
    "\t\n",
    "\tnew_pos = pos[~pos['ID'].isin(old_ids)]\n",
    "\tnew_neg = neg[~neg['ID'].isin(old_ids)]\n",
    "\tnew_unc = unc[~unc['ID'].isin(old_ids)]\n",
    "\t\n",
    "\t# add new rows to test and test_u\n",
    "\tif not new_pos.empty or not new_neg.empty:\n",
    "\t\ttest = pd.read_csv(PATH+'test.csv', sep='\\t')\n",
    "\t\ttest = pd.concat([test,new_pos,new_neg]).sort_values(by='ID')\n",
    "\t\ttest.to_csv(PATH+'test.csv', sep='\\t', index=None)\n",
    "\n",
    "\t\ttest_u = pd.read_csv(PATH+'test_u.csv', sep='\\t')\n",
    "\t\ttest_u = pd.concat([test_u,new_pos,new_neg]).sort_values(by='ID')\n",
    "\t\ttest_u.to_csv(PATH+'test_u.csv', sep='\\t', index=None)\n",
    "\n",
    "\tif not new_unc.empty:\n",
    "\t\ttest_u = pd.read_csv(PATH+'test_u.csv', sep='\\t')\n",
    "\t\ttest_u = pd.concat([test_u,new_unc]).sort_values(by='ID')\n",
    "\t\ttest_u.to_csv(PATH+'test_u.csv', sep='\\t', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
