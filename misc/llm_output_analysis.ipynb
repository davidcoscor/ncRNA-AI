{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_duplicates(folder, common_name, k):\n",
    "\tlog_files = [file for _,_,files in os.walk(folder) for file in files if common_name in file]\n",
    "\tlog_files.sort()\n",
    "\n",
    "\tduplicates = defaultdict(list)\n",
    "\tfor file in log_files[:k]:\n",
    "\t\twith open(os.path.join(folder,file), 'r') as f:\n",
    "\t\t\tentries = json.load(f)\n",
    "\t\t\n",
    "\t\tfor entry in entries:\n",
    "\t\t\tif entry['model'] == 'gemma2:27b':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tindex = (entry['model'], entry['n_shots'])\n",
    "\t\t\tinfo = {\n",
    "\t\t\t\t'precision': entry['metrics']['precision'],\n",
    "\t\t\t\t'recall': entry['metrics']['recall'],\n",
    "\t\t\t\t'f1': entry['metrics']['f1'],\n",
    "\t\t\t\t'invalid%': entry['invalid%'],\n",
    "\t\t\t\t'runtime': entry['runtime']['single'],\n",
    "\t\t\t\t'n_queries': entry['n_queries']\n",
    "\t\t\t}\n",
    "\t\t\tduplicates[index].append(info)\n",
    "\n",
    "\treturn dict(duplicates)\n",
    "\n",
    "\n",
    "def analyse_prompts(file):\n",
    "\tduplicates = defaultdict(list)\n",
    "\twith open(file, 'r') as f:\n",
    "\t\tentries = json.load(f)\n",
    "\t\n",
    "\tprompts = list(set([entry['prompt'] for entry in entries]))\n",
    "\n",
    "\tfor entry in entries:\n",
    "\t\tif entry['model'] in ['mistral','mixtral','mixtral:8x22b']:\n",
    "\t\t\tcontinue\n",
    "\t\tindex = (entry['model'], prompts.index(entry['prompt']))\n",
    "\t\tinfo = {\n",
    "\t\t\t'n_shots': entry['n_shots'],\n",
    "\t\t\t'precision': entry['metrics']['precision'],\n",
    "\t\t\t'recall': entry['metrics']['recall'],\n",
    "\t\t\t'f1': entry['metrics']['f1'],\n",
    "\t\t\t'invalid%': entry['invalid%'],\n",
    "\t\t\t'runtime': entry['runtime']['single'],\n",
    "\t\t\t'n_queries': entry['n_queries']\n",
    "\t\t}\n",
    "\t\tduplicates[index].append(info)\n",
    "\n",
    "\tmetrics = ['precision','recall','f1','invalid%','runtime','n_queries']\n",
    "\tresults = defaultdict(list)\n",
    "\tmodels = []\n",
    "\tfor model,infos in duplicates.items():\n",
    "\t\tmodels.append(model)\n",
    "\t\tfor metric in metrics:\n",
    "\t\t\tvalues = [info[metric] for info in infos]\n",
    "\t\t\tresults[metric].append(round(np.mean(values),4))\n",
    "\t\t\tresults[metric+'_dev'].append(round(np.std(values),4))\n",
    "\t\tresults['n_shots'].append(infos[0]['n_shots'])\n",
    "\n",
    "\tresults = pd.DataFrame(results, index=models)\n",
    "\treturn results, prompts\n",
    "\n",
    "\n",
    "def analyse_duplicates(duplicates):\n",
    "\tmetrics = ['precision','recall','f1','invalid%','runtime','n_queries']\n",
    "\t\n",
    "\tresults = defaultdict(list)\n",
    "\tmodels = []\n",
    "\tfor model,infos in duplicates.items():\n",
    "\t\tmodels.append(model)\n",
    "\t\tfor metric in metrics:\n",
    "\t\t\tvalues = [info[metric] for info in infos]\n",
    "\t\t\tresults[metric].append(round(np.mean(values),4))\n",
    "\t\t\tresults[metric+'_dev'].append(round(np.std(values),4))\n",
    "\n",
    "\tresults = pd.DataFrame(results, index=models)\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def include_std_devs(df):\n",
    "\tcols = [col for col in df.columns if '_dev' not in col]\n",
    "\tnew_cols = {}\n",
    "\tfor col in cols:\n",
    "\t\tnew_cols[col] = df[col].apply(lambda x: f\"{x:.3f}\")+' Â± '+df[col+'_dev'].apply(lambda x: f\"{x:.3f}\")\n",
    "\treturn pd.DataFrame(new_cols)\n",
    "\n",
    "\n",
    "def plot_results_prompt(results_df, palette):\n",
    "\tdata = {\n",
    "\t\t'Model':[idx[0] for idx in results_df.index],\n",
    "\t\t'Prompt':[idx[1] for idx in results_df.index],\n",
    "\t\t'F1':results_df['f1'].tolist()\n",
    "\t}\n",
    "\tdf = pd.DataFrame(data)\n",
    "\n",
    "\tplt.figure(figsize=(8, 5))\n",
    "\n",
    "\tsns.lineplot(data=df, x='Prompt', y='F1', hue='Model', marker='o', palette=palette, estimator=None)\n",
    "\n",
    "\t# plt.title()\n",
    "\tplt.xticks([i for i in range(5)])\n",
    "\t# plt.yticks([i*0.001 for i in range(800, 1001, 25)])\n",
    "\tplt.xlabel('Prompt Used')\n",
    "\tplt.ylabel('F1-score')\n",
    "\tplt.grid(True)\n",
    "\tplt.legend(title='Model', bbox_to_anchor=(1.01, 1.02), loc='upper left')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def plot_results_inv(results_df, palette):\n",
    "\tdata = {\n",
    "\t\t'Model':[idx[0] for idx in results_df.index],\n",
    "\t\t'Shots':[idx[1] for idx in results_df.index],\n",
    "\t\t'Invalid %':results_df['invalid%'].tolist()\n",
    "\t}\n",
    "\tdf = pd.DataFrame(data)\n",
    "\n",
    "\tplt.figure(figsize=(6, 3))\n",
    "\n",
    "\tsns.lineplot(data=df, x='Shots', y='Invalid %', hue='Model', marker='o', palette=palette, estimator=None)\n",
    "\n",
    "\t# plt.title()\n",
    "\t# plt.xticks([i for i in range(2,11,2)])\n",
    "\t# plt.yticks([i*0.001 for i in range(800, 1001, 25)])\n",
    "\tplt.xlabel('Number of Examples')\n",
    "\tplt.ylabel('Invalid %')\n",
    "\tplt.grid(True)\n",
    "\tplt.legend(title='Model', bbox_to_anchor=(1.01, 1.03), loc='upper left')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def plot_results(results_df, palette, dotted=False):\n",
    "\tdata = {\n",
    "\t\t'Model':[idx[0] for idx in results_df.index],\n",
    "\t\t'Shots':[idx[1] for idx in results_df.index],\n",
    "\t\t'Queries':results_df['n_queries'].tolist(),\n",
    "\t\t'F1':results_df['f1'].tolist()\n",
    "\t}\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tif dotted:\n",
    "\t\tdf2 = df[df['Model'].isin(dotted)]\n",
    "\t\tdf = df[~df['Model'].isin(dotted)]\n",
    "\t\n",
    "\tplt.figure(figsize=(8, 5))\n",
    "\n",
    "\tsns.lineplot(data=df, x='Shots', y='F1', hue='Model', marker='o', palette=palette, estimator=None)\n",
    "\tif dotted:\n",
    "\t\tsns.lineplot(data=df2, x='Shots', y='F1', linestyle='dotted', hue='Model', marker='o', palette=palette, estimator=None)\n",
    "\t# sns.scatterplot(data=df, x='Shots', y='F1', hue='Model', size='Queries', sizes=(20, 200), legend='brief', palette=palette, zorder=4)\n",
    "\n",
    "\tbest = df.loc[df['F1'].idxmax()]\n",
    "\tbest_label = f\"Best:\\nF1: {best['F1']:.3f}\\nmodel: {best['Model']}\\nn_shots: {best['Shots']}\\nn_queries: {best['Queries']}\"\n",
    "\t# sns.scatterplot(data=pd.DataFrame([best]), x='Shots', y='F1', s=100, color='black', marker='+', label=best_label, zorder=5)\n",
    "\n",
    "\t# plt.title()\n",
    "\tplt.xticks([i for i in range(2,11,2)])\n",
    "\tplt.yticks([i*0.001 for i in range(790, 1001, 25)])\n",
    "\tplt.xlabel('Number of Examples')\n",
    "\tplt.ylabel('F1-Score')\n",
    "\tplt.grid(True)\n",
    "\tplt.legend(title='Model', bbox_to_anchor=(1.01, 1.02), loc='upper left')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = load_duplicates('./outputs/chained', 'control', 4)\n",
    "control_results = analyse_duplicates(control)\n",
    "# control_results.sort_values(by=['f1','invalid%'], ascending=False, inplace=True)\n",
    "# display(control_results)\n",
    "\n",
    "models = list(set([run[0] for run in control]))\n",
    "models.sort()\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(models))\n",
    "palette = {model:palette[i] for i,model in enumerate(models)}\n",
    "plot_results(control_results, palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain = load_duplicates('./outputs/chained', 'unc', 4)\n",
    "uncertain_results = analyse_duplicates(uncertain).sort_values(by=['invalid%'], ascending=False)\n",
    "\n",
    "chained = load_duplicates('./outputs/chained', 'chained', 4)\n",
    "chained_results = analyse_duplicates(chained)\n",
    "chained_results['invalid_u%'] = uncertain_results['invalid%']\n",
    "chained_results['invalid_u%_dev'] = uncertain_results['invalid%_dev']\n",
    "# chained_results.drop(['precision','precision_dev','recall','recall_dev','runtime','runtime_dev'], axis=1, inplace=True)\n",
    "# chained_results.sort_values(by=['invalid_u%','f1','invalid%'], ascending=False, inplace=True)\n",
    "drop_models = ['mixtral','mixtral:8x22b','llama3','gemma']\n",
    "# chained_results = chained_results[~chained_results.index.map(lambda x: x[0] in drop_models)]\n",
    "# display(chained_results)\n",
    "plot_results(chained_results, palette, dotted=drop_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tables\n",
    "# Control\n",
    "control_table = control_results.drop(['n_queries', 'n_queries_dev'], axis=1)\n",
    "control_table = include_std_devs(control_table)\n",
    "# control_table.to_latex('./control.tex')\n",
    "\n",
    "# Chained\n",
    "models = []\n",
    "uncs_p = []\n",
    "uncs_p_devs = []\n",
    "for model,infos in chained.items():\n",
    "\tmodels.append(model)\n",
    "\tvalues = [(67-info['n_queries'])/67*100 for info in infos]\n",
    "\tuncs_p.append(round(np.mean(values),4))\n",
    "\tuncs_p_devs.append(round(np.std(values),4))\n",
    "\n",
    "chained_table = chained_results.drop(['n_queries', 'n_queries_dev'], axis=1)\n",
    "chained_table.loc[models, 'unc%'] =  uncs_p\n",
    "chained_table.loc[models, 'unc%_dev'] =  uncs_p_devs\n",
    "chained_table = include_std_devs(chained_table)\n",
    "chained_table = chained_table[['precision','recall','f1','invalid%','runtime','unc%','invalid_u%']]\n",
    "# chained_table.to_latex('./chained.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = load_duplicates('./outputs/new', 'n_shots_0.json', 1)\n",
    "n_shots_results = analyse_duplicates(n_shots)\n",
    "plot_results_inv(n_shots_results, palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "uncertain_val = load_duplicates('./outputs/validation', 'unc', 4)\n",
    "uncertain_val_results = analyse_duplicates(uncertain_val)\n",
    "validation = load_duplicates('./outputs/validation', 'chained', 4)\n",
    "validation_results = analyse_duplicates(validation)\n",
    "\n",
    "validation_results['invalid_u%'] = uncertain_val_results['invalid%']\n",
    "validation_results['invalid_u%_dev'] = uncertain_val_results['invalid%_dev']\n",
    "\n",
    "models = []\n",
    "uncs_p = []\n",
    "uncs_p_devs = []\n",
    "for model,infos in validation.items():\n",
    "\tmodels.append(model)\n",
    "\tvalues = [(58-info['n_queries'])/58*100 for info in infos]\n",
    "\tuncs_p.append(round(np.mean(values),4))\n",
    "\tuncs_p_devs.append(round(np.std(values),4))\n",
    "\n",
    "validation_results.loc[models, 'unc%'] =  uncs_p\n",
    "validation_results.loc[models, 'unc%_dev'] =  uncs_p_devs\n",
    "validation_results = include_std_devs(validation_results)\n",
    "validation_results = validation_results[['precision','recall','f1','invalid%','runtime','unc%','invalid_u%']]\n",
    "\n",
    "display(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_results, prompts = analyse_prompts('./outputs/new/prompts.json')\n",
    "# prompts_results.sort_index(inplace=True)\n",
    "# plot_results_prompt(prompts_results, palette)\n",
    "# display(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_json(old, new):\n",
    "# \twith open(old, 'r') as f:\n",
    "# \t\traw_entries = f.read().split(f'\\n\\n{\"=\"*100}\\n')\n",
    "\t\n",
    "# \tentries = []\n",
    "# \tfor raw_entry in raw_entries:\n",
    "# \t\tif '> Model:' not in raw_entry: # badly formatted\n",
    "# \t\t\tcontinue\n",
    "\t\t\n",
    "# \t\tdef read_field(field):\n",
    "# \t\t\ttry: return raw_entry.split(field)[1].split('\\n')[0].strip()\n",
    "# \t\t\texcept: return None\n",
    "\n",
    "# \t\tmodel = read_field('> Model:')\n",
    "# \t\tn_shots = read_field('> N-shots:')\n",
    "# \t\toptions = read_field('> Options:')\n",
    "# \t\tprompt = raw_entry.split(\"> Prompt:\\n'''\\n\")[1].split(\"\\n'''\")[0]\n",
    "# \t\ttry:\n",
    "# \t\t\texamples = raw_entry.split(\"> Examples:\\n\")[1].split(\"> Prediction time\")[0].split('> > ')[1:]\n",
    "# \t\texcept IndexError:\n",
    "# \t\t\texamples = []\n",
    "# \t\truntime = float(read_field('> Prediction time (average):').split(' s')[0])\n",
    "# \t\tN = int(read_field('> Number of predictions:'))\n",
    "# \t\tf1 = float(read_field('> > F1:'))\n",
    "# \t\tprc = float(read_field('> > Precision:'))\n",
    "# \t\trcl = float(read_field('> > Recall:'))\n",
    "\n",
    "# \t\tdef read_mislabel(field):\n",
    "# \t\t\ttry: return ast.literal_eval(raw_entry.split(field)[1].split('): ')[1])\n",
    "# \t\t\texcept: return []\n",
    "\n",
    "# \t\tfp = read_mislabel('> FP (')\n",
    "# \t\tfn = read_mislabel('> FN (')\n",
    "# \t\tinv = read_mislabel('> Invalid (')\n",
    "# \t\tmislabels = {}\n",
    "# \t\tfor name,insts in zip(['fp','fn','invalid'],[fp,fn,inv]):\n",
    "# \t\t\tn = len(insts)\n",
    "# \t\t\tif n == 0: continue\n",
    "# \t\t\tinsts = [{'query':inst[0],'model_response':inst[1]} for inst in insts]\n",
    "# \t\t\tmislabel = {'N':n, 'instances':insts}\n",
    "# \t\t\tmislabels[name] = mislabel\n",
    "\n",
    "# \t\tif len(mislabels) == 0:\n",
    "# \t\t\tmislabels = None\n",
    "\n",
    "# \t\tN = N + len(inv)\n",
    "\n",
    "# \t\tentry = {\n",
    "# \t\t\t'model': model,\n",
    "# \t\t\t'n_shots': n_shots,\n",
    "# \t\t\t'options': options,\n",
    "# \t\t\t'prompt': prompt,\n",
    "# \t\t\t'examples': examples,\n",
    "# \t\t\t'n_queries': N,\n",
    "# \t\t\t'runtime': {'total': runtime, 'single': runtime/N},\n",
    "# \t\t\t'invalid%': (len(inv)/N)*100,\n",
    "# \t\t\t'metrics': {'precision': prc, 'recall': rcl, 'f1': f1},\n",
    "# \t\t\t'mislabels': mislabels\n",
    "# \t\t}\n",
    "# \t\tentries.append(entry)\n",
    "\t\n",
    "# \twith open(new, 'w') as f:\n",
    "# \t\tjson.dump(entries, f, indent=1)\n",
    "\n",
    "# def convert_n_preds_to_invalid(log_file):\n",
    "# \twith open(log_file, 'r') as f:\n",
    "# \t\traw_entries = json.load(f)\n",
    "\t\n",
    "# \tentries = []\n",
    "# \tfor raw_entry in raw_entries:\n",
    "# \t\ttry:\n",
    "# \t\t\tN_inv = raw_entry['mislabels']['inv']['N']\n",
    "# \t\texcept KeyError:\n",
    "# \t\t\tN_inv = 0\n",
    "# \t\tN = raw_entry['n_preds'] + N_inv\n",
    "\t\t\n",
    "# \t\tif raw_entry['model'] != 'mixtral:8x22b':\n",
    "# \t\t\tcontinue\n",
    "\n",
    "# \t\tdisplay(raw_entry)\n",
    "# \t\tentry = {\n",
    "# \t\t\t'model': raw_entry['model'],\n",
    "# \t\t\t'n_shots': raw_entry['n_shots'],\n",
    "# \t\t\t'options': raw_entry['options'],\n",
    "# \t\t\t'prompt': raw_entry['prompt'],\n",
    "# \t\t\t'examples': raw_entry['examples'],\n",
    "# \t\t\t'n_queries': N,\n",
    "# \t\t\t'runtime': raw_entry['runtime'],\n",
    "# \t\t\t'invalid%': (N_inv/N)*100,\n",
    "# \t\t\t'metrics': raw_entry['metrics'],\n",
    "# \t\t\t'mislabels': raw_entry['mislabels']\n",
    "# \t\t}\n",
    "# \t\tdisplay(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_json('./outputs/new/n_shots_0.txt', './outputs/new/n_shots_0.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
